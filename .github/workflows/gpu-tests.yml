name: GPU Hardware Tests

on:
  workflow_dispatch:
    inputs:
      gpu_type:
        description: 'GPU type to test'
        required: true
        default: 'A100'
        type: choice
        options:
          - A100
          - V100
          - H100
      test_mode:
        description: 'Test mode'
        required: true
        default: 'baseline'
        type: choice
        options:
          - baseline
          - frequency-validation
          - power-profiling
          - edp-optimization
      runner_label:
        description: 'Self-hosted runner label (optional)'
        required: false
        default: 'gpu-enabled'
        type: string
  schedule:
    # Run monthly GPU validation tests
    - cron: '0 3 1 * *'

env:
  PYTHON_VERSION: "3.11"

permissions:
  contents: read
  actions: read

jobs:
  gpu-frequency-validation:
    name: GPU Frequency Validation
    runs-on: ${{ github.event.inputs.runner_label || 'self-hosted' }}
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Check GPU availability and permissions
        run: |
          if command -v nvidia-smi &> /dev/null; then
            echo "âœ… nvidia-smi found"
            if nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits; then
              echo "âœ… GPU access confirmed"
            else
              echo "âŒ GPU access denied or no GPUs found"
              exit 78
            fi
          else
            echo "âŒ nvidia-smi not found - skipping GPU tests"
            exit 78
          fi
          
      - name: Check DCGMI availability
        continue-on-error: true
        run: |
          if command -v dcgmi &> /dev/null; then
            echo "âœ… DCGMI found"
            dcgmi discovery --list || echo "âš ï¸ DCGMI discovery failed"
          else
            echo "â„¹ï¸ DCGMI not found - will use nvidia-smi fallback"
          fi
          
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
      - name: Validate hardware frequency support
        run: |
          GPU_TYPE="${{ github.event.inputs.gpu_type || 'A100' }}"
          echo "Testing GPU type: $GPU_TYPE"
          
          if nvidia-smi -q -d SUPPORTED_CLOCKS > actual_clocks.txt 2>&1; then
            echo "âœ… Successfully queried GPU clock information"
          else
            echo "âš ï¸ Failed to query detailed clock information"
            nvidia-smi --query-gpu=clocks.max.graphics,clocks.max.memory --format=csv,noheader,nounits > actual_clocks.txt || echo "Basic clock query failed"
          fi
          
          python3 -c "
          import sys
          from hardware.gpu_info import get_gpu_info
          
          gpu_type = '$GPU_TYPE'
          try:
              gpu_info = get_gpu_info(gpu_type)
              configured_freqs = gpu_info.get_available_frequencies()
              print(f'âœ… Configured {gpu_type} frequencies: {len(configured_freqs)}')
              print(f'ðŸ“Š Range: {min(configured_freqs)}-{max(configured_freqs)} MHz')
              print(f'âœ… All >= 510 MHz: {all(f >= 510 for f in configured_freqs)}')
              
              try:
                  with open('actual_clocks.txt', 'r') as f:
                      clocks_output = f.read()
                  print('âœ… Successfully read actual GPU clock data')
                  
                  import re
                  graphics_freqs = re.findall(r'Graphics\\\\s*:\\\\s*(\\\\d+)\\\\s*MHz', clocks_output)
                  if graphics_freqs:
                      actual_freqs = [int(f) for f in graphics_freqs if int(f) >= 510]
                      print(f'ðŸ“Š Actual frequencies >= 510 MHz: {len(actual_freqs)}')
                      if actual_freqs:
                          print(f'ðŸ“Š Actual range: {min(actual_freqs)}-{max(actual_freqs)} MHz')
                      
                      overlap = set(configured_freqs) & set(actual_freqs)
                      if overlap:
                          print(f'âœ… Found {len(overlap)} overlapping frequencies')
                      else:
                          print('âš ï¸ No exact frequency overlap - may be expected for different GPU variants')
                  else:
                      print('â„¹ï¸ Could not parse detailed frequency information')
                      
              except Exception as e:
                  print(f'âš ï¸ Error processing clock data: {e}')
                  
          except Exception as e:
              print(f'âŒ Error validating GPU configuration: {e}')
              sys.exit(1)
          "
          
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gpu-test-results-${{ github.event.inputs.gpu_type || 'A100' }}
          path: |
            actual_clocks.txt
            *.log
          retention-days: 30
          
  hardware-info-collection:
    name: Hardware Information Collection
    runs-on: ${{ github.event.inputs.runner_label || 'self-hosted' }}
    timeout-minutes: 15
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Collect comprehensive hardware information
        run: |
          echo "=== System Information ===" > hardware_info.txt
          uname -a >> hardware_info.txt 2>&1 || echo "uname failed" >> hardware_info.txt
          
          echo -e "\n=== CPU Information ===" >> hardware_info.txt
          lscpu >> hardware_info.txt 2>&1 || echo "lscpu not available" >> hardware_info.txt
          
          echo -e "\n=== Memory Information ===" >> hardware_info.txt
          free -h >> hardware_info.txt 2>&1 || echo "free command failed" >> hardware_info.txt
          
          echo -e "\n=== GPU Information ===" >> hardware_info.txt
          if command -v nvidia-smi &> /dev/null; then
            nvidia-smi >> hardware_info.txt 2>&1
            echo -e "\n=== Detailed GPU Query ===" >> hardware_info.txt
            nvidia-smi --query-gpu=index,name,memory.total,memory.free,driver_version,pstate,temperature.gpu,power.draw,power.limit --format=csv >> hardware_info.txt 2>&1
          else
            echo "nvidia-smi not available" >> hardware_info.txt
          fi
          
          echo -e "\n=== CUDA Information ===" >> hardware_info.txt
          if command -v nvcc &> /dev/null; then
            nvcc --version >> hardware_info.txt 2>&1
          else
            echo "CUDA compiler not available" >> hardware_info.txt
          fi
          
      - name: Upload hardware information
        uses: actions/upload-artifact@v4
        with:
          name: hardware-info-${{ github.run_id }}
          path: hardware_info.txt
          retention-days: 90
